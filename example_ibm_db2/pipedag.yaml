instances:
  __any__:
    network_interface: "127.0.0.1"
    auto_table:
      - "pandas.DataFrame"
      - "polars.DataFrame"
      - "polars.LazyFrame"
      - "sqlalchemy.sql.expression.TextClause"
      - "sqlalchemy.sql.expression.Selectable"
      - "pydiverse.transform.Table"

    fail_fast: true
    instance_id: pipedag_default

    # Attention: For disable_kroki: false, stage and task names might be sent to the kroki_url.
    #   You can self-host kroki if you like:
    #   https://docs.kroki.io/kroki/setup/install/
    #   You need to install optional dependency 'pydot' for any visualization
    #   URL to appear.
    disable_kroki: true
    kroki_url: "https://kroki.io"

    table_store:
      class: "pydiverse.pipedag.backend.table.SQLTableStore"
      args:
        url: "db2+ibm_db://db2inst1:password@localhost:50000/testdb"
        schema_prefix: "{instance_id}_"
        create_database_if_not_exists: false

        print_materialize: true
        print_sql: true

        # the following settings are all optional but may improve performance
        # when chosen correctly (discuss table spaces with the DB Admin)
        strict_materialization_details: false
        default_materialization_details: "no_compression"
        materialization_details:
          # materialization details as defined in the following like "value_compression"
          # or "table_space" can be activated either
          # on Stage level: Stage(name, materialization_details="value_compression") or
          # on Table level: dag.Table(name, materialization_details="value_compression")
          __any__:
            compression: [ "COMPRESS YES ADAPTIVE" ]
          no_compression:
            compression: ""
          value_compression:
            compression: "VALUE COMPRESSION"
          static_compression:
            compression: "COMPRESS YES STATIC"
          adaptive_value_compression:
            compression: [ "COMPRESS YES ADAPTIVE", "VALUE COMPRESSION" ]
          table_space:
            table_space_data: "S1"
            table_space_index: "S2"
            table_space_long: "S3"

      hook_args:
        sql:
          cleanup_annotation_action_on_success: false
          cleanup_annotation_action_intermediate_state: false

      local_table_cache:
        store_input: true
        store_output: true
        use_stored_input_as_cache: true
        class: "pydiverse.pipedag.backend.table.cache.ParquetTableCache"
        args:
          base_path: "/tmp/pipedag/table_cache"

    stage_commit_technique: READ_VIEWS  # db2 does not support schema renaming

    blob_store:
      class: "pydiverse.pipedag.backend.blob.FileBlobStore"
      args:
        base_path: "/tmp/pipedag/blobs"

    lock_manager:
      class: "pydiverse.pipedag.backend.lock.DatabaseLockManager"

    orchestration:
      class: "pydiverse.pipedag.engine.SequentialEngine"
